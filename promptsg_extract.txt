

===== PAGE 1 =====
A Pedestrian is Worth One Prompt: Towards Language Guidance Person
Re-Identification
Zexian Yang1,2
Dayan Wu1*
Chenming Wu3
Zheng Lin1
Jingzi Gu1
Weiping Wang1,2
1Institute of Information Engineering, Chinese Academy of Sciences
2School of Cyber Security, University of Chinese Academy of Sciences
3 Baidu Inc
{yangzexian,wudayan,linzheng,gujingzi,wangweiping}@iie.ac.cn,wuchenming@baidu.com
Abstract
Extensive advancements have been made in person ReID
through the mining of semantic information. Nevertheless,
existing methods that utilize semantic-parts from a single
image modality do not explicitly achieve this goal. White-
ness the impressive capabilities in multimodal understand-
ing of Vision Language Foundation Model CLIP, a recent
two-stage CLIP-based method employs automated prompt
engineering to obtain specific textual labels for classify-
ing pedestrians. However, we note that the predefined soft
prompts may be inadequate in expressing the entire vi-
sual context and struggle to generalize to unseen classes.
This paper presents an end-to-end Prompt-driven Semantic
Guidance (PromptSG) framework that harnesses the rich
semantics inherent in CLIP. Specifically, we guide the model
to attend to regions that are semantically faithful to the
prompt.
To provide personalized language descriptions
for specific individuals, we propose learning pseudo to-
kens that represent specific visual contexts.
This design
not only facilitates learning fine-grained attribute informa-
tion but also can inherently leverage language prompts dur-
ing inference. Without requiring additional labeling efforts,
our PromptSG achieves state-of-the-art by over 10% on
MSMT17 and nearly 5% on the Market-1501 benchmark.
The codes will be available at https://github.com/
YzXian16/PromptSG
1. Introduction
Person Re-Identification (ReID) is a crucial research area
in computer vision that focuses on identifying individuals
across different camera views or time instances [4, 44, 45,
57], which is a sub-task of image-based retrieval. Features
of the same individual, as captured by various cameras, are
prone to alterations due to changes in lighting, background,
and body posture. Consequently, the effectiveness of a so-
*Corresponding author.
Figure 1. Transformer visualization [2] of attention maps. (a)
Original images, (b) CLIP-ReID, (c) Our method w/o inversion,
and (d) Our method guided by the composed prompts captures
both the exact semantic parts and the external appearance details.
Visual
Encoder
Text
Encoder
Input imageinversion ùë∫‚àó
‚ÄúA photo of a ùë∫‚àóperson‚Äù
Composed Text Prompt
CLS Token
Patch Token
Cross-attention
Map
reweight
Cross Attention
Figure 2. The core idea of our method. Our method inverts in-
put images into pseudo-word tokens S‚àó, which are then composed
into a textual prompt to describe the specific visual context. The
attention map of patch tokens is further controlled by the seman-
tics of the textual prompt.
phisticated ReID model fundamentally depends on its capa-
bility to learn discriminative features that are impervious to
camera-specific variations, thereby enhancing the model‚Äôs
capacity to generalize to previously unseen classes.
Modern ReID models, constructed upon uni-modal ar-
chitectures such as the Convolutional Neural Network
(CNN) [22] or Vision Transformer (ViT) [14, 19, 35, 40],
have made significant advancements within the field.
A
substantial portion of these solutions focus on the extrac-
tion of pertinent regions to rectify misalignment issues.
These strategies are dedicated to the extraction of semantic
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
17343


===== PAGE 2 =====
data, such as the human body structure, primarily facilitated
through the integration of identity classification [33, 43]
and metric learning [24, 55]. However, it is worth noting
that these attention regions generally highlight only specific
locally discriminative parts without explicit semantic con-
trol. When a distinct mask or skeleton direction is necessi-
tated [27, 31], the need for additional, labor-intensive, and
time-consuming manual labeling becomes inevitable.
Large-scale Vision Language (VL) models, exemplified
by Contrastive Language-Image Pre-Training (CLIP) [26],
have recently shown remarkable abilities in reasoning
across multi-modal data. CLIP model, when provided with
text prompts such as ‚ÄòA photo of a [CLASS]‚Äô, displays ex-
ceptional zero-shot classification performance at the image
level. This leads to a question: Can we further direct at-
tention to regions of interest through natural language de-
scriptions, such as ‚ÄòA photo of a person‚Äô? However, due
to the resulting visual representation lacking fine-grained
information necessary for distinguishing between identi-
ties, integrating CLIP straightforwardly into person ReID
is non-trivial. Additionally, the query ‚ÄòA photo of a per-
son‚Äô presents a challenge due to the absence of specific de-
scriptors, thereby lacking a personalized prompt for indi-
vidual identification. The pioneering CLIP-ReID [21] in-
troduces automated prompt engineering on CLIP by incor-
porating additional ID-wise learnable vectors customized
for specific identities. Particularly, CLIP-ReID employs a
two-stage training process that first optimizes the learnable
vectors with the frozen CLIP model, and then restricts the
image encoder with the learned textual descriptions. How-
ever, the disentangled usage, i.e., only the visual embedding
is utilized during inference, renders the learned soft prompts
ineffective for unseen prompts. As a result, the attention re-
gions potentially do not entirely encompass the body part,
and may inadvertently include background elements, such
as cars and additional pedestrians captured in the scene, as
illustrated in the first two examples in Fig. 1(b). In addi-
tion, even though CLIP-ReID adheres to training objectives
aimed at vision-language alignment, such predefined soft
prompts may not be sufficient to characterize the entire vi-
sual context of the specified pedestrian.
In this paper, we propose Prompt-driven Semantic
Guidance (PromptSG), that aims to streamline the two-
stage pipeline by leveraging the foundational CLIP model
effectively and efficiently. As outlined in Fig. 2, our core in-
sight is straightforward: we strive to activate CLIP‚Äôs cross-
modal comprehension using explicit language prompts, and
the regions extracted can then be fine-tuned to enhance
semantic discriminativeness. Specifically, given a textual
prompt, we refine the patch tokens by injecting cross-
attention maps, determining which patch attends to the cor-
responding semantics. Following this rationale, we revisit
the fundamental issue that the term ‚Äòperson‚Äô serves as a
coarse descriptor, lacking personalized language descrip-
tions for individual identities. Beyond semantic informa-
tion related to the ‚Äòperson‚Äô, appearance information is also
crucial for identification purposes [5]. While semantic in-
formation aids the model in better body part localization,
appearance information further refines the focus on an indi-
vidual‚Äôs attire. Hence, we employ the textual inversion tech-
nique [10], which learns to represent visual context through
unique token. We use a lightweight inversion network that
maps the image to a pseudo-token. This pseudo-token can
then be incorporated into the textual prompt, creating an
embedding that closely mirrors the original image. Com-
pared to CLIP-ReID, our solution offers two primary ad-
vantages: 1) The textual prompt emphasizes regions in the
image via a cross-attention map, capturing the precise se-
mantic part (Fig. 1(c)), and can also be utilized for unseen
classes during inference. 2) The model can learn the per-
sonal token of the query image in an end-to-end manner,
providing more detailed guidance specific to an identity
(Fig. 1(d)). Importantly, our proposed method is free,
i.e. there is no need to supply additional information,
such as masks, bounding boxes, or precise descriptions.
We summarize the contribution of this paper as follows.
‚Ä¢ Leveraging the exceptional multi-modal reasoning capa-
bilities of CLIP, we propose PromptSG, a novel frame-
work for the person ReID task. This approach uniquely
utilizes language prompts, providing explicit assistance to
the visual encoder in efficiently capturing semantic infor-
mation.
‚Ä¢ To create a more personalized description for the individ-
ual, we propose learning to represent the specific, more
detailed appearance attributes, by employing the inver-
sion network.
‚Ä¢ Without any additional labelling efforts, PromptSG sur-
passes previous SOTA method [21] by over 10% on
the MSMT17 dataset. It also exhibits superior perfor-
mance on the Market-1501 benchmark, surpassing pre-
vious SOTA method [46] by nearly 5%.
2. Related Work
Person Re-identification remains an important yet chal-
lenging task due to the subtle inter-class differences. To
learn more discriminative representations, a category of
CNN-based techniques has primarily concentrated on op-
timizing the distance metric via metric learning [15, 33, 34,
37, 38]. Recognizing the importance of semantic informa-
tion, a substantial body of research [3, 23, 31, 43] explores
the use of attention mechanisms, which guide the network
to extract attention-aware features for body parts. For ex-
ample, AAnet [36] adopts a unified learning framework that
incorporates attribute attention maps through extra attribute
labels. Pioneering work TransReID [14] introduces a self-
attention-based architecture, Vision Transformer (ViT) [8],
17344


===== PAGE 3 =====
Âú®Ê≠§Â§ÑÈîÆÂÖ•ÂÖ¨Âºè„ÄÇ
Visual
Encoder
Text
Encoder
Inversion 
Network
‚ÄúA photo of a ùë∫‚àóperson‚Äù
Training Stages
Inference
ùíçùíë
ùíóùüèùíóùüêùíóùüë
‡∑•ùíó
Cross Attention
2x
ùë≥ùë∫ùíñùíëùë™ùíêùíè
ùë≥ùë∞ùë´+ ùë≥ùëªùíìùíäùíëùíçùíÜùíï
Frozen
Tunable
Text/Image 
embedding
A photo of a person
ùë∏
ùë≤
ùëΩ
Visual
Encoder
Multimodal
Interaction Module
Identification
Gallery Embeddings
Query
Ôºà
)
ùëë
ùë∏
ùë≤ùëª
ùëΩ
=
ùíÅ
Self Attention
Feed Forward
efficiency
accuracy
OR
ùë∫‚àó
Simplified Prompt
Composed Prompt
Figure 3. Overview of our framework. PromptSG learns pseudo token S‚àófrom the specific visual embedding, and the visual encoder learns
semantic faithful representations with the guidance of language prompts that occur in the Multimodal Interaction Module.
for advancing ReID tasks. DCAL [56] proposes to implic-
itly extract the local features through a global-local cross-
attention mechanism. However, these methods solely ap-
ply attention mechanisms to the visual modality, and the
lack of explicit language guidance potentially constrains
their performance. The work most relevant to ours, CLIP-
ReID [21], is the first to utilize vision-language pre-training
model CLIP in ReID task. However, CLIP-ReID fails to
leverage the linguistic capability of the text encoder in CLIP
during inference, since the ID-specific learnable tokens only
influence the seen identities.
Large-scale vision-language pre-training model con-
nects the image representation with text embedding in a
shared embedding space, has demonstrated effectiveness
across a wide range of uni-modal and multimodal down-
stream tasks.
These include classification [6, 48], im-
age captioning [25], and cross-modal retrieval [11, 16, 32,
42, 49]. Foundational VL models, such as CLIP, usually
undergo training on extensive image-text pairs with con-
trastive learning objectives. This foundational pre-training
provides the model with strong open-vocabulary classi-
fication capabilities.
Inherited from prompt learning in
NLP [18], CoOp [54] proposes to explore learnable prompt
optimization on few-shot classification.
Following this
soft prompt approach, CLIP-ReID pioneers the adaptation
of CLIP for person ReID by classifying images into ID-
specific prompts.
Differing from CLIP-ReID, which fo-
cuses on vision-language alignment, our goal is to exploit
rich semantic information from language to explicitly con-
trol the weights assigned to each patch or region, and im-
prove the two-stage framework by directly inverting images
into the language latent space.
Textual Inversion, originally for personalized text-to-
image generation [10], is a learning approach that aims to
discover new pseudo-words in the word-embedding space.
These pseudo-words are capable of encapsulating both the
overall visual content and intricate visual details. Recently,
the application of textual inversion has expanded to zero-
shot composed image retrieval task [1, 29]. In these studies,
a textual inversion network is typically pre-trained using ex-
tensive unlabeled image datasets. In this work, we stand out
as the first to apply this learning paradigm to person ReID
without any additional training data.
3. Preliminary
Contrastive Language-Image Pre-training (CLIP)
undergoes pre-training on a large corpus of image-text
pairs, aligning visual and linguistic representations within
a shared space through the matching of images with their
corresponding text descriptions. Specifically, CLIP consists
of a visual encoder V(¬∑) and a text encoder T (¬∑). The vi-
sual encoder V(¬∑) takes an image x ‚ààRH√óW √óC as input.
The text encoder T (¬∑) takes a tokenized textual descrip-
tion t ‚ààRN√óD as input, where N,D are the text‚Äôs length
and token feature dimension respectively. The pre-training
objective is based on self-supervised contrastive learning,
which minimizes cosine distance for matched image-text
pairs. For the downstream tasks such as classification, the
description of j-th class is typically obtained through the
17345


===== PAGE 4 =====
hand-crafted prompt, e.g., ‚ÄòA photo of a [CLASS]‚Äô. There-
fore, the probability of image x being classified as class y
can be computed as follows:
P(y|x) =
exp(sim(V(x), T (ty))/œÑ)
PK
j=1exp(sim(V(x), T (tj))/œÑ)
.
(1)
where œÑ denotes the temperature, and sim(a, b) =
a¬∑b
‚à•a‚à•2‚à•b‚à•2
is the cosine similarity.
A simple approach to applying CLIP to person ReID in-
volves substituting the linear classifier with image-to-text
classification. However, given that labels in ReID tasks are
solely index-based, there are no specific words to repre-
sent different persons. To tackle this challenge, CLIP-ReID
crafts the prompt as ‚ÄòA photo of a [Xi]1[Xi]2[Xi]3...[Xi]M
person‚Äô, where [Xi]m, m ‚àà{1, ..., M} represents a set
of ID-specific learnable tokens for the i-th ID. Neverthe-
less, CLIP-ReID optimizes ID-specific prompts exclusively
bound to training IDs, it overlooks the chance to fully ex-
ploit the open-vocabulary capabilities inherent in CLIP.
4. Method
An overview of our framework is depicted in Fig. 3.
Starting with the visual embeddings derived from CLIP‚Äôs
visual encoder, our approach employs an inversion network
to learn pseudo tokens that encapsulate the visual context.
Following this, an interaction between visual and textual
modalities is facilitated in the interaction module, leading
to the final re-weighted representations.
During the in-
ference phase, we are presented with two options for tex-
tual inputs: an efficiency-driven simplified prompt and an
accuracy-driven composed prompt. Note that the text en-
coder is frozen in our entire framework.
4.1. Learning the Personalized ID-Specific Prompt
As suggested by prior research, the word-embedding
space possesses sufficient expressiveness to encapsulate ba-
sic image concepts [7]. However, the inherent limitation
lies in the pre-defined prompts in CLIP-ReID, which can
only capture limited attributes and may not fully encapsu-
late the visual context. Contrarily, we propose learning the
pseudo token by textual inversion technique that aligns with
the context of the query image.
Let fŒ∏(¬∑) denote an inversion network parameterized by
Œ∏, our goal is to invert the global visual embedding v from
visual space of CLIP, represented as v ‚ààV , into a pseudo
token s‚àó‚ààT‚àóby fŒ∏(v) = s‚àó, where T‚àóindicates the token
embedding space. Subsequently, this pseudo token can be
integrated into natural language sentences. As such, the lan-
guage prompt for the input image is structured as ‚ÄòA photo
of a s‚àóperson‚Äô. It is worth noting that this pseudo-token
bears no relationship to an actual word but functions as a
representation in the token embedding space. An input lan-
guage prompt undergoes a tokenization process, resulting
in several tokens. The tokenized prompt, denoted as tp, can
be fed into the text encoder of CLIP to obtain text embed-
ding lp = T (tp). To ensure that the learned pseudo-token
effectively tells the context of the image, one can follow to
the reconstruction objective of textual inversion by the sym-
metric contrastive loss, which is formulated as follows:
Li2t = 1
N
N
X
n=1
log
exp(sim(vn, lp)/œÑ)
PN
i=1exp(sim(vn, li)/œÑ)
,
(2)
Lt2i = 1
N
N
X
n=1
log
exp(sim(ln, vp)/œÑ)
PN
i=1exp(sim(ln, vi)/œÑ)
.
(3)
In this context, vi or li represents the i-th image/text em-
bedding in a batch. lp is the corresponding prompt embed-
ding for vn and is constructed in a manner analogous to vp.
The underlying mechanism is grounded in the principle
of cycle-consistency, wherein a pseudo token tends to faith-
fully represent the context of the image only when the text
features closely align with corresponding image features.
However, the contrastive loss fails to handle cases where
images with the same ID are supposed to share the same
appearance. Therefore, we aim to encourage the pseudo to-
ken to capture visual details exclusive to the same identity.
To this end, we exploit the symmetric supervised contrastive
loss as follows:
LSupCon = LSup
i2t + LSup
t2i .
(4)
LSup
i2t = 1
N
N
X
n=1
X
p+‚ààP (i)
log
exp(sim(vn, lp+)/œÑ)
PN
i=1exp(sim(vn, li)/œÑ)
,
(5)
LSup
t2i = 1
N
N
X
n=1
X
p+‚ààP (i)
log
exp(sim(ln, vp+)/œÑ)
PN
i=1exp(sim(ln, vi)/œÑ)
.
(6)
where P(i) represents the positive samples related to vn, ln.
4.2. Prompt-driven Semantic Guidance
We refine the language prompt by incorporating the
pseudo-token that is linked to the identity, enhancing its
ability to convey a more specific visual context for the im-
age. Our commitment extends to meticulously directing the
image feature through language.
At the core of our ap-
proach lies the idea of semantic guidance, wherein we ex-
plicitly determine which region of the image aligns with the
language prompt. Intuitively, image patches corresponding
to the semantic ‚Äúperson‚Äù should inherently have substantial
influence to facilitate discrimination. As opposed to the in-
teraction between patches in self-attention layers within a
17346


===== PAGE 5 =====
single modality. Based on this observation, we explore a
patch-to-prompt interaction that occurs in multi-modality.
In particular, we employ a language-guided cross-
attention module, which uses the textual embedding as
query and the patch-wise embedding of the visual encoder
as key and value. More formally, given a pair of image
and prompt (x, tp), we first feed the image x into the vi-
sual encoder, yielding in a sequence of patch embeddings
{Àúv, v1, ..., vM}. Here, Àúv denotes the global visual embed-
ding, while remaining vi, i ‚àà[1, M] belong to the local
patch embeddings. In a similar vein, the prompt is fed into
the text encoder to derive the text embedding lp. Subse-
quently, the text embedding is projected onto a query matrix
Q and patch embeddings are projected to a key matrix K
and a value matrix V , via three different linear-projection
layers.
As such, the patch-to-prompt interaction can be
achieved by:
A(Q, K, V ) = Softmax(QKT
‚àö
d
)V.
(7)
This interaction aggregates the attention map to highlight
the regions of high semantic response. Drawing from multi-
modal fusion methods [9], we incorporate two transformer
blocks following the cross-attention layer to derive final
representations. Ultimately, we utilize the standard ReID
loss, i.e., the triplet loss and identity classification loss [12],
to optimize our framework.
LID = 1
K
K
X
j=1
yj log pj,
(8)
LTriplet = max(dp ‚àídn + m, 0),
(9)
where pj is the prediction probability for the j-th class, and
m denotes the margin.
4.3. Optimization and Inference
Taining optimization. In summary, the overall objective
function for our framework is formulated as:
L = LTriplet + LID + ŒªLSupCon.
(10)
Similar to CLIP-ReID, the final hidden states of the vi-
sion transformer, in conjunction the preceding two layer
states, are also employed to calculate LTriplet.
Improved inference efficiency. Our approach involves
the query-specific pseudo token with the textual prompt,
which essentially doubles the inference time compared to
using only the visual encoder. Fortunately, our empirical
findings suggest that providing only ‚ÄòA photo of a person‚Äô
as a simplified guideline yields comparable results. In this
way, there will be no increase in the inference time caused
by the text encoder.
Dataset
#ID
Images
Cams
Market-1501
1,501
32,668
6
MSMT17
4,101
126,441
15
DukeMTMC
1,404
36,411
8
CHUK03-NP
1,467
13,164
2
Table 1. The statistics of dataset in our experiments
5. Experiments
5.1. Experimental Setting
Datasets and Evaluation Protocols.
To evaluate
and compare various methods, four extensive person re-
identification datasets Market-1501 [51], MSMT17 [41],
DukeMTMC [52] and CUHK03-NP [22] are exploited.
Dataset stats are in Tab. 1. In line with conventions in the
ReID community [13], two commonly used metrics, i.e.,
mean Average Precision (mAP) and Rank-1(R-1) accuracy,
are used to evaluate the performance.
Implementation Details. In alignment with prior re-
search, we employ both ResNet-50 and ViT-B/16 Pre-
trained from CLIP as our visual encoder and a pre-trained
text encoder, i.e., CLIP text Transformer. Our framework
additionally features a random-initialized inversion network
and a multimodal interaction module. The inversion net-
work is a lightweight model employing a three-layered
MLP of 512-dimensional hidden state. A Batch Normal-
ization (BN) layer [17] is placed after the last state of the
network. The batch size is configured to 64, encompassing
16 identities with 4 images per identity. All input images
are resized to 256 √ó 128. We use the Adam optimizer with
a learning rate of 5e-6 for the visual encoder, whereas the
learning rate for random-initialized modules is set to 5e-5.
We find Œª in Eq. (10) is not sensitive and performs well
across a broad range, thus we consistently set Œª = 0.5 for
all datasets. The model is trained for 60 epochs, with a
learning rate decay factor of 0.1 for every 20 epochs. The
entire framework is implemented using PyTorch and runs
on a single NVIDIA RTX3090 GPU with 24GB VRAM.
Baseline. Most existing approaches are built upon the
strong ReID baseline presented in [24]. Specifically, they
employ an ImageNet-21k pre-trained CNN model or ViT
as the backbone and incorporate ID loss and triplet loss as
crucial components. In contrast, our baseline model devi-
ates by leveraging the pre-trained CLIP model and we fine-
tune the visual encoder of CLIP by directly applying the
two commonly-used losses.
5.2. Comparison with State-of-the-art Methods
We benchmark PromptSG against the current state-of-
the-art, which can generally be divided into three cat-
egories: CNN-based, ViT-based, and CLIP-based meth-
ods. Tab. 2 summarizes the main results on four widely
17347


===== PAGE 6 =====
Backbone
Method
Reference
Market-1501
MSMT17
DukeMTMC
CUHK03-NP
mAP
R-1
mAP
R-1
mAP
R-1
mAP
R-1
CNN-based method
ResNet50
OSNeT [53]
ICCV‚Äô19
84.9
94.8
52.9
78.7
73.5
88.6
-
-
ISP [57]
ECCV‚Äô20
84.9
94.2
-
-
75.6
86.9
74.1
76.5
RGA-SC [50]
CVPR‚Äô20
88.4
96.1
57.5
80.3
-
-
77.4
81.1
CDNet [20]
CVPR‚Äô21
86.0
95.1
54.7
78.9
76.8
88.6
-
-
CAL [28]
ICCV‚Äô21
87.0
94.5
56.2
79.5
76.4
87.6
-
-
ALDER* [47]
TIP‚Äô21
88.9
95.6
59.1
82.5
78.9
89.9
78.7
81.0
LTReID* [39]
TMM‚Äô22
86.9
94.7
58.6
81.0
80.4
90.5
80.3
82.1
CLIP-based method
Baseline
88.1
94.7
60.7
82.1
79.3
88.6
77.6
79.1
CLIP-ReID [21]
AAAI‚Äô23
89.8
95.7
63.0
84.4
80.7
90.0
78.2
79.4
PromptSG
Ours
91.8
96.6
68.5
86.0
80.4
90.2
79.8
80.5
ViT-based method
ViT-B/16
TransReID [14]
ICCV‚Äô21
88.9
95.2
67.4
85.3
82.0
90.7
79.6
81.7
DCAL [56]
CVPR‚Äô22
87.5
94.7
64.0
83.1
80.1
89.0
-
-
AAformer [58]
TNNLS‚Äô23
88.0
95.4
65.6
84.4
80.9
90.1
79.0
80.3
PHA [46]
CVPR‚Äô23
90.2
96.1
68.9
86.1
-
-
83.0
84.5
CLIP-based method
Baseline
86.4
93.3
66.1
84.4
80.0
88.8
80.0
80.5
CLIP-ReID [21]
AAAI‚Äô23
89.6
95.5
73.4
88.7
82.5
90.0
81.6
80.9
PromptSG
Ours
94.6
97.0
87.2
92.6
81.6
91.0
83.1
85.1
Table 2. Comparison with the state-of-the-art models on Market-1501, MSMT17, DukeMTMC, and CUHK03-NP (labeled) datasets. The
superscript star* indicates that the image is resized to a resolution exceeding 256x128. All results are reported without re-ranking. Color
Red and blue: the best and second-best results.
used person ReID datasets. We observe that our proposed
PromptSG attains the best results and sets a new state-of-
the-art performance. Remarkably, PromptSG achieves over
10% improvement on MSMT17 and nearly 5% on Market-
1501, surpassing previous state-of-the-art results.
Compared with ViT-based method. Pioneering work
TransReID [14] sets a strong baseline for the ViT-based
method by leveraging the potentials of the transformer.
Building upon this groundwork, PHA [46] further enhances
the preservation of key high-frequency elements in images.
In contrast to existing ViT-based methods that only cap-
ture the patch-wise uni-modal information, our PromptSG
method demonstrates that the interaction of different modal-
ities can improve the performance of individual modalities.
Compared with CLIP-based method.
Compared
with the competing CLIP-based method CLIP-ReID, our
PromptSG outperforms it by 5.0%/1.5% and 13.8%/3.9%
mAP/Rank-1 on Market-1501 and MSMT17 datasets when
taking ViT-B/16 as visual backbone. A key distinction be-
tween CLIP-ReID and our approach resides in the composi-
tion of the query-specific pseudo-token. Our results further
underscore that incorporating textual information during the
inference process can also enhance performance.
Compared with CNN-based method. To ensure a fair
comparison, we also implement PromptSG with a ResNet-
50 backbone. Apart from LTReID [39] that utilize higher
resolution images, our method consistently surpasses other
methods by a significant margin, especially on the most
challenging person ReID dataset, MSMT17.
This high-
lights the robustness and superiority of our approach across
various architectures.
5.3. Ablation Study
In the following, we conduct an ablation study on
the essential elements of PromptSG on Market-1501 and
MSMT17 datasets, and all the experiments are conducted
on the ViT-B/16 backbone.
Contributions from Different Components. To assess the
contribution of various components, we conduct ablation
experiments by removing one component at a time. Re-
call that LSup
i2t and LSup
t2i are the supervised contrastive losses
in Eq. (2), Eq. (3) respectively, and MIM denotes the mul-
timodal interaction module. Comparing rows b) and c) with
a), we see a similar conclusion where the removal of text-
to-image or image-to-text contrastive loss leads to a decent
improvement on both datasets. Further comparing rows a)
and d), we observe that the removal of semantic information
leads to a larger decrease than solely removing ID-specific
appearance information. Notably, as seen in row a), our full
model, PromptSG, utilizes both semantic and appearance
17348


===== PAGE 7 =====
Components
Market-1501
MSMT17
LSup
i2t
LSup
t2i
MIM
mAP
R-1
mAP
R-1
a)
!
!
!
94.6
97.0
87.2
92.6
b)
!
!
92.8
96.7
85.2
91.9
c)
!
!
93.0
96.7
84.5
90.2
d)
!
!
89.4
95.3
71.4
87.3
Table 3. Ablation study on the effectiveness of each component of
PromptSG on Market-1501 and MSMT17.
Method
Market-1501
MSMT17
mAP
R-1
mAP
R-1
Training w/ composed
94.6
97.0
87.2
92.6
Training w/o composed
92.0
96.3
85.3
91.6
Table 4.
Ablation of training with or without composing the
pseudo token on Market-1501 and MSMT17.
language supervision during training, achieving a substan-
tial improvement of over a point. The overall conclusion
supports that language guidance, through both semantic and
appearance cues, plays a crucial role in improving the per-
formance of our model.
Ablation Study on the Personalized Prompt.
To bet-
ter understand whether the learned pseudo tokens s‚àócan
provide more granular guidance for learning visual embed-
dings, we train a strong baseline model, where the textual
prompt dose not composed with the s‚àóduring training and
testing, but instead relies on the simplified prompt ‚ÄúA photo
of a person‚Äù for semantic guidance. Note that we will not
use the symmetric supervised contrastive loss in this case.
Results in Tab. 4 imply that composing the s‚àóhas a sig-
nificant impact on the overall performance. When s‚àóis re-
moved from the training process, the performance decreases
by 1.9% to 2.6% in terms of mAP. Although we focus on
the uni-modal re-identification task, the above formulation
could potentially be applied to multimodal test sets, such
as text-to-image person retrieval by composing the image
feature with the text to achieve better alignment.
Ablation Study on the Interaction Module. We analyze
the impact of different designs of the interaction module on
performance and inference speed, as well as the impact of
not using a composed prompt during inference. Notably,
personalized prompts are consistently included during the
training. As shown in Tab. 5, without an attention mod-
ule (w/o attention module), the model achieves a baseline
performance, with inference speed being dependent solely
on the visual encoder. Introducing a single cross-attention
layer (+1 cross-layer) shows a notable performance im-
provement, indicating the positive effect of incorporating
a cross-layer design. Notably, performances can be stably
improved with more self-attention layers, but at the cost of
Traning Model
Inference Model
FPS ‚Üë
mAP
Visual
Encoder
Text
Encoder
w/o attention module
!
-
1x
89.4
+ 1 cross-layer
!
-
0.95x
91.1
+ 1 cross & 1 self-layer
!
-
0.91x
93.0
+ 1 cross & 2 self-layer
!
!
0.48x
94.6
+ 1 cross & 2 self-layer
!
-
0.88x
94.1
Table 5.
The impact of various interaction modules and effi-
ciency comparison with different inference models on Market-
1501. Cross and self means cross-attention and self-attention, re-
spectively. FPS denotes the quantity of images processed by the
model in one second.
Method
#Params
#Params
%CLIP
Training Times ‚Üì
(a) Market-1501
CLIP-ReID
89M
0.71
4689s
1x
PromptSG
94M
0.75
2417s
0.51x
(b) MSMT17
CLIP-ReID
90M
0.73
12904s
1x
PromptSG
94M
0.75
6108s
0.47x
Table 6. Comparison of training times and the number of parame-
ters on Market-1501 and MSMT17. #Params denotes the number
of learnable parameters in the whole framework. All models are
evaluated on a single 3090Ti GPU.
lower inference efficiency. Furthermore, our analysis illu-
minates the impact of employing a composed prompt during
the inference phase, revealing that when we follow the same
procedure as the training stage‚Äîcomposing text with query
images‚Äîthe Frames Per Second (FPS) is only 0.48 times
that of the baseline. This is expected as we need to pass
through two encoders for each query. However, we empir-
ically discovered that using a fixed prompt ‚ÄúA photo of a
person‚Äù for all queries may not lead to significant perfor-
mance degradation, and it does not compromise efficiency.
Therefore, one could opt for this version to achieve a more
favorable balance between accuracy and efficiency.
Comparison of training efficiency. In order to showcase
the efficiency of our proposed approach, we carry out a
comparative analysis between our one-stage PromptSG and
the two-stage CLIP-ReID method, focusing on the number
of learnable parameters and training speed. The details of
this comparison are provided in Tab. 6. In terms of training
parameters, on top of CLIP, CLIP-ReID incorporates an ad-
ditional of parameters mainly through the ID-wise learnable
prompt, our approach primarily extends through a fixed-size
mapping network and an interaction module. Despite CLIP-
ReID having 2%-4% fewer parameters than ours on two
datasets, it may experience continuous growth in parame-
17349


===== PAGE 8 =====
(a)
(b)
(c)
(d)
(a)
(b)
(c)
(d)
(a)
(b)
(c)
(d)
(a)
(b)
(c)
(d)
Market-1501
MSMT17
Figure 4. Transformer visualization of attention maps. (a) Original images, (b) CLIP-ReID, (c) PromptSG without composed training, (d)
PromptSG. We see our method is effective in simultaneously focusing on the semantic clues and exploring more discriminative parts.
ters in scenarios with a higher number of classes or evolv-
ing dynamics. In contrast, PromptSG demonstrates stronger
robustness in the number of parameters and achieves signif-
icant faster training speed. It can achieve a speedup of ap-
proximately 2 times faster during training compared to the
two-stage method.
6. Qualitative Analysis
To have an intuitive understanding and validate the ef-
fectiveness of our method, we conduct a qualitative analysis
where visualization of attention maps is presented in Fig. 4.
Specifically, we exhibit examples from Market-1501 and
MSMT17 datasets, each with two training images and two
gallery images. We carefully selected some challenging
examples, including those with complex backgrounds or
images depicting multiple individuals. To gain a better
insight into the regions of interest attended by the model
in zero-shot scenarios, we do not use the common proto-
col GramCAM [30], as it needs the class-prediction scores
and might be considered less suitable for Transformer-
type backbones. Following [21], we use the Transformer-
interpretability method in [2].
We compare our (d) PromptSG with (b) CLIP-ReID and
(c) PromptSG without image-composed training. It can be
seen that our method exhibits significant effectiveness, as
it adeptly captures semantic information while also concen-
trating on more detailed appearance details. For example,
in the first row of the Market-1501 dataset, the attention
map of CLIP-ReID is susceptible to interference from back-
ground elements like ‚Äúcar‚Äù. On the other hand, PromptSG
w/o composed training tends to emphasize semantic infor-
mation related to the ‚Äòperson,‚Äô focusing on the location of
the head, arms, and legs. In contrast, our method goes be-
yond this by also exploring appearance features, such as
identifying individuals wearing hats or carrying backpacks.
Finally, in the first row examples of MSMT17, where ad-
ditional pedestrians appear in the image, our method excels
in effectively filtering out unnecessary pedestrians, while
CLIP-ReID fails.
7. Conclusion
In this paper, we propose PromptSG, a simple yet effec-
tive framework that exploits the foundational model CLIP
for the person ReID task. We show that language guidance
is an effective way to adapt pre-trained multimodal mod-
els for the uni-modal retrieval tasks. Through leveraging
the aligned multi-modal latent space provided by CLIP, the
textual prompt ‚ÄùA photo of a person‚Äù can naturally address
the challenge of the visual encoder in its struggle to capture
semantic information. To probe more fine-grained appear-
ance features, we incorporate an inversion network to learn
pseudo tokens that describe the image context.
Discussion and Limitation.
Despite the consider-
able potential of language prompt learning in ReID tasks,
prompt learning in the vision branch remains a largely un-
tapped area. Fine-tuning the visual encoder for strong su-
pervised performance may lead to poor zero-shot general-
ization. We hope our work can inspire future research on
fully unleashing the potential of large foundation models in
challenging ReID tasks.
Acknowledgments This work was supported by the
National Key R&D Program of China under Grant
2022YFB3103500, the National Natural Science Foun-
dation of China under Grants 62106258, 62006242 and
62202459, and the China Postdoctoral Science Founda-
tion under Grant 2022M713348 and 2022TQ0363, and
Young Elite Scientists Sponsorship Program by CAST
(2023QNRC001) and BAST (NO.BYESS2023304).
17350
